{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/Image-Captioning/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkVD7rvu9p0-"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.17.0 && pip install torch==2.2.0 && pip install torchvision==0.17.0 && pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import vocab\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.io import decode_image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from os import path\n",
        "from random import randint\n",
        "from collections import Counter\n",
        "from google.cloud import storage\n",
        "from tqdm import tqdm\n",
        "from evaluate import load\n",
        "from torch.func import vmap\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "qWjjgq-E-Bk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA Optimizations"
      ],
      "metadata": {
        "id": "aRwdtrbpEpoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True # more efficient highly-accurate data format\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  torch.backends.cuda.enable_flash_sdp(True) # efficient version of scaled dot product attention comptuation\n",
        "  torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "  torch.backends.cuda.enable_math_sdp(True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yX_54CKp96E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d-h2hEV70Wb"
      },
      "outputs": [],
      "source": [
        "encoder = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True) # loading MobileNetV2.\n",
        "\n",
        "encoder.classifier = nn.Identity() # removing the final classification layer to retrieve the feature map. Feature map: [1, 1280]\n",
        "encoder.to(device) # moving to CUDA if possible"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in encoder.parameters(): # freezing the encoder since we're not training it\n",
        "  param.requires_grad = False # avoid computing gradients for brevity"
      ],
      "metadata": {
        "id": "7A3k3UUx-M-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0h9CFD09l8o"
      },
      "outputs": [],
      "source": [
        "class GRU_Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, feature_map_size=1280, embed_size=256, hidden_size=512, num_layers=2, vocab_size=10000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feature_map_size = feature_map_size\n",
        "    self.embed_size = embed_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size) # word --> embedding (vector representation)\n",
        "    self.proj = nn.Linear(in_features=self.feature_map_size, out_features=self.hidden_size) # dim(feature space) --> dim(hidden state) to draw caption-related information from the raw images\n",
        "    self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True) # embed_size --> hidden_size\n",
        "    self.fc = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size) # hidden state vector -> vocabulary vector (in hidden state vector space, the vector is not interpretable hence it needs to go to vocabulary vector space)\n",
        "\n",
        "  def generate(self, feature_map, bos_token, eos_token, max_len=10):\n",
        "    batch_size = feature_map.size(0)\n",
        "    bos_token = self.embed(bos_token).unsqueeze(1)\n",
        "    h = self.proj(feature_map).unsqueeze(0) # initial hidden state\n",
        "\n",
        "    last_word = bos_token\n",
        "    caption = []\n",
        "\n",
        "    for i in range(max_len): # don't have the entire caption yet, so need to loop until its generated\n",
        "      output, h = self.gru(last_word, h) # passing the last word generated through the GRU layer to get the next word\n",
        "      logits = self.fc(output) # now in vocabulary vector space\n",
        "      word = torch.argmax(word, dim=1) # argmaxxing to get the most probable predicted word\n",
        "      caption.append(word) # adding this word to the caption generated so far\n",
        "\n",
        "      if torch.all(word == eos_token): # comparing word and eos token across the various dimensions\n",
        "        break # exit if reached end of caption\n",
        "\n",
        "      last_word = self.embed(word).unsqueeze(1) # shifting the last_word pointer to the right\n",
        "\n",
        "    return torch.stack(caption, dim=1) # formatting the caption correctly before returning it\n",
        "\n",
        "  def forward(self, feature_map, words):\n",
        "    batch_size = feature_map.size(0)\n",
        "    embedding = self.embed(words) # returns a vector representation of a word\n",
        "    h0 = self.proj(feature_map).unsqueeze(0) # initializes the hidden state by projecting the feature map onto the hidden state dimensional space\n",
        "    h0 = h0.reshape(self.num_layers, batch_size, self.hidden_size) # gru expects hidden state in a certain format\n",
        "    output, _ = self.gru(embedding, h0) # teacher-forcing with the correct captions\n",
        "    logits = self.fc(output) # going from hidden state vector space --> vocabulary vector space\n",
        "\n",
        "    return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE3zVz0Vwp5-"
      },
      "outputs": [],
      "source": [
        "class MiniCoco(Dataset):\n",
        "\n",
        "  def __init__(self, json_file, root_dir, split, transform=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.full_data = pd.read_json(json_file)\n",
        "    self.data = self.full_data[\"images\"]\n",
        "    self.split = split\n",
        "    self.counter = Counter() # counting the # of occurances of a particular word in a sentence\n",
        "    self.captions = [] # nested list with all the captions for each sample\n",
        "\n",
        "    if self.split == \"train\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"restval\"]\n",
        "    elif self.split == \"val\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"val\"]\n",
        "    elif self.split == \"test\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"test\"]\n",
        "    else:\n",
        "      raise Exception(\"Invalid split\")\n",
        "\n",
        "    self.length = len(self.data)\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    if self.split == \"train\": # only want to store captions for train -- during test/val model should be generating without knowing any ground truth\n",
        "      for sample in range(len(self.data)): # iterating over all samples in the train dataset\n",
        "        cap = [] # captions for particular sample\n",
        "\n",
        "        for j in range(len(self.data[sample][\"sentences\"])): # iterating over the various captions provided for each sample\n",
        "          caption = self.data[sample][\"sentences\"][j]\n",
        "          token = caption[\"tokens\"]\n",
        "          self.counter.update(token) # keeping track of the frequency of each token\n",
        "          cap.append(token)\n",
        "\n",
        "        self.captions.append(cap)\n",
        "    else:\n",
        "      self.captions = None # if self.captions is empty, indexing it can fail. Setting it to None fixes this\n",
        "\n",
        "    special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "    self.vocab = vocab(self.counter, specials=special_tokens, special_first=True, min_freq=2)\n",
        "    self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "\n",
        "    for i in range(len(self.captions)):\n",
        "      for j in range(len(self.captions[i])):\n",
        "        self.captions[i][j] = self.encode(self.captions[i][j]) # mapping each caption in the nested list to an integer via encode()\n",
        "\n",
        "  def encode(self, text):\n",
        "    return [self.vocab[\"<bos>\"]] + [self.vocab.get_stoi()[s] for s in text] + [self.vocab[\"<eos>\"]]\n",
        "\n",
        "  def itos(self, tens):\n",
        "    return ' '.join(self.vocab.get_itos()[i] for i in tens[1:-1]) # return space-seperated string composed from sequence of integers\n",
        "\n",
        "  def decode(self, ints):\n",
        "    if ints.dim() == 1:\n",
        "      return self.itos(ints)\n",
        "\n",
        "    return [self.itos(seq) for seq in ints]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # train images should only have 1 caption (leads to faster convergence when teacher-forcing during training)\n",
        "    captions = self.captions[index][randint(0, len(self.captions[index]) - 1)] if self.split == \"train\" else self.captions[index]\n",
        "\n",
        "    # storing the image into memory as a torch tensor\n",
        "    image_name = path.join(self.root_dir, self.data[index][\"filename\"])\n",
        "    image = decode_image(image_name, mode=\"RGB\")\n",
        "\n",
        "    return image, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyf7gs9i6rIV",
        "outputId": "c3c6091e-9d30-4966-8a91-bec8e9318fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "transform_encoder = v2.Compose(\n",
        "    [\n",
        "        v2.Resize((224, 224)),\n",
        "        v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "8-6iUJPPq2Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9stlTrjRqH6i"
      },
      "outputs": [],
      "source": [
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "\n",
        "  client = storage.Client(project=\"Image Captioning\")\n",
        "  bucket = client.bucket(bucket_name)\n",
        "  blob = bucket.blob(source_blob_name)\n",
        "  blob.download_to_filename(destination_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxmAMNnTrIEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2426084-5ca7-46d5-b52b-560e9720e0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/test2014.zip\", \"/content/test2014.zip\")\n",
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/train2014.zip\", \"/content/train2014.zip\")\n",
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/val2014.zip\", \"/content/val2014.zip\")\n",
        "download_blob(\"img-captioning\", \"archive.zip\", \"/content/archive.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rVWta3qpwIH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!unzip /content/test2014.zip -d /content/test2014/ && unzip /content/train2014.zip -d /content/train2014/ && unzip /content/archive.zip -d /content/archive/ && !unzip /content/val2014.zip -d /content/val2014/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/test2014.zip /content/train2014.zip /content/val2014.zip /content/archive.zip"
      ],
      "metadata": {
        "id": "kWfX3ZuZv15w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/archive && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3OgMABv9-9",
        "outputId": "d1e94749-ca3d-4680-a155-a4b10e8c931d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_coco.json  dataset_flickr30k.json  dataset_flickr8k.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = GRU_Decoder()\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "id": "KvHDkdK1-ifr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = \"/content/archive/dataset_coco.json\"\n",
        "root_train_dir = \"/content/train2014/train2014/\"\n",
        "root_test_dir = \"/content/test2014/test2014/\"\n",
        "root_val_dir = \"/content/val2014/val2014/\"\n",
        "\n",
        "train_data = MiniCoco(json_file, root_train_dir, \"train\")\n",
        "test_data = MiniCoco(json_file, root_test_dir, \"test\")\n",
        "val_data = MiniCoco(json_file, root_val_dir, \"val\")"
      ],
      "metadata": {
        "id": "TdU9QiP6116j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(data):\n",
        "  images, captions = zip(*data)\n",
        "  images = torch.stack(images, dim=0)\n",
        "\n",
        "  captions = pad_sequence(captions, batch_first=True, padding_value=train_data.vocab[\"<pad>\"])\n",
        "\n",
        "  return images, captions"
      ],
      "metadata": {
        "id": "jNEpKk2YborJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=pad)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=pad)\n",
        "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=pad)"
      ],
      "metadata": {
        "id": "NBC3Nbm913Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adagrad(decoder.parameters())"
      ],
      "metadata": {
        "id": "mcA1QrcNz4r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "bertscore = load(\"bertscore\")"
      ],
      "metadata": {
        "id": "F306irHEB9AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "  running_loss = 0\n",
        "\n",
        "  decoded_predictions = []\n",
        "  decoded_captions = []\n",
        "\n",
        "  for batch_num, data in enumerate(train_dataloader):\n",
        "    images, captions = data # corresponds to image, caption\n",
        "\n",
        "    images = images.to(device)\n",
        "    captions = captions.to(device)\n",
        "    # images: [B, C, H, W]\n",
        "\n",
        "    sliced_captions = captions[:, :-1] # removing eos token\n",
        "    optimizer.zero_grad() # zeroing gradients because they accumulate\n",
        "\n",
        "    input_tensor = transform_encoder(images) # applying transformation and adding batch dimension\n",
        "    feature_map = encoder(input_tensor) # getting a feature map\n",
        "    outputs = decoder(feature_map, sliced_captions)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 2) # [batchsize, caption]\n",
        "    decoded_predictions.append(train_data.decode(predicted))\n",
        "    decoded_captions.append(train_data.decode(captions))\n",
        "\n",
        "    loss = loss_fn(outputs, sliced_captions)\n",
        "    loss.backward()\n",
        "    running_loss += loss.item()\n",
        "    optimizer.step()\n",
        "\n",
        "  avg_loss = running_loss / len(train_dataloader)\n",
        "  acc_total = sum(bertscore.compute(predictions=decoded_predictions, references=decoded_captions)[\"f1\"]) # getting the f1 score using bertscore\n",
        "  avg_acc = acc_total / len(train_dataloader)\n",
        "\n",
        "  return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "lJeRo02qHz0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate():\n",
        "  decoded_predictions = []\n",
        "  decoded_captions = []\n",
        "\n",
        "  for vdata in val_dataloader:\n",
        "    vimages, vcaptions = vdata\n",
        "\n",
        "    vimages = vimages.to(device)\n",
        "    vcaptions = vcaptions.to(device)\n",
        "\n",
        "    input_tensor = transform_encoder(vimages)\n",
        "    feature_map = encoder(input_tensor)\n",
        "    voutputs = decoder.generate(feature_map, train_data.vocab[\"<bos>\"], train_data.vocab[\"<eos>\"])\n",
        "\n",
        "    _, vpredicted = torch.max(voutputs.data, 2)\n",
        "    decoded_predictions.append(val_data.decode(vpredicted))\n",
        "    decoded_captions.append(val_data.decode(vcaptions))\n",
        "\n",
        "  vacc_total = sum(bertscore.compute(predictions=decoded_predictions, references=decoded_captions)[\"f1\"]) # getting the f1 score using bertscore\n",
        "  vacc = vacc_total / len(val_dataloader)\n",
        "\n",
        "  return vacc"
      ],
      "metadata": {
        "id": "C7tAiLUEIj49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = -1\n",
        "loop = tqdm(range(epochs))\n",
        "\n",
        "for epoch in loop:\n",
        "  decoder.train()\n",
        "  avg_loss, train_acc = train_one_epoch()\n",
        "\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    vacc = validate()\n",
        "\n",
        "  loop.set_description(f\"Avg Loss: {avg_loss} \\t Train Acc: {train_acc} \\t Val Acc: {vacc}\")\n",
        "\n",
        "  if vacc > best_acc:\n",
        "    torch.save(decoder.state_dict(), \"decoder.pt\")\n",
        "    best_acc = vacc\n"
      ],
      "metadata": {
        "id": "-XRud_Xv5AVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  decoded_predictions = []\n",
        "  decoded_captions = []\n",
        "\n",
        "  for tdata in test_dataloader:\n",
        "    timages, tcaptions = tdata\n",
        "\n",
        "    timages = timages.to(device)\n",
        "    tcaptions = tcaptions.to(device)\n",
        "\n",
        "    input_tensor = transform_encoder(timages)\n",
        "    feature_map = encoder(input_tensor)\n",
        "    toutputs = decoder.generate(feature_map, train_data.vocab[\"<bos>\"], train_data.vocab[\"<eos>\"])\n",
        "\n",
        "    _, tpredicted = torch.max(toutputs.data, 2)\n",
        "    decoded_predictions.append(test_data.decode(tpredicted))\n",
        "    decoded_captions.append(test_data.decode(tcaptions))\n",
        "\n",
        "  tacc_total = sum(bertscore.compute(predictions=decoded_predictions, references=decoded_captions)[\"f1\"]) # getting the f1 score using bertscore\n",
        "  tacc = tacc_total / len(test_dataloader)\n",
        "\n",
        "  return tacc"
      ],
      "metadata": {
        "id": "rUSY7L1NyLRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  decoder.eval()\n",
        "  print(test())"
      ],
      "metadata": {
        "id": "N15x6idyyFMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe2layqRtpeiK/ATeiLAjv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}