{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/Image-Captioning/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & installation"
      ],
      "metadata": {
        "id": "hUJu6kp8ZDUK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkVD7rvu9p0-"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext && pip install torch && pip install torchvision && pip install bert-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import vocab\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.io import decode_image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from os import path\n",
        "from random import randint\n",
        "from collections import Counter\n",
        "from google.cloud import storage\n",
        "from tqdm import tqdm\n",
        "from bert_score import score\n",
        "from torch.func import vmap\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from numpy import mean"
      ],
      "metadata": {
        "id": "qWjjgq-E-Bk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "The following is a list of the major hyperparameters used in this project."
      ],
      "metadata": {
        "id": "5ONirS6yYWGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHSIZE = 32 # batch size of data passed to encoder\n",
        "EPOCHS = 20 # training epochs\n",
        "MIN_FREQUENCY = 2 # minimium times a token must appear to be included in the vocabulary\n",
        "FEATURE_MAP_SIZE = 1280 # size of the feature map\n",
        "EMBED_SIZE = 256 # dimension of the word embedding vector space\n",
        "HIDDEN_SIZE = 512 # dimension of the hidden state vector space\n",
        "NUM_LAYERS = 2 # number of layers in the GRU\n",
        "VOCAB_SIZE = 10000 # dimension of the vocabulary vector space\n",
        "MAX_LEN = 10 # max length of caption generated\n",
        "DROPOUT_RATE = 0.1 # Proportion of neurons to be randomly deactivated each forward pass"
      ],
      "metadata": {
        "id": "b8Cid5vPYeVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA Optimizations"
      ],
      "metadata": {
        "id": "aRwdtrbpEpoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True # more efficient highly-accurate data format\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  torch.backends.cuda.enable_flash_sdp(True) # efficient version of scaled dot product attention comptuation\n",
        "  torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "  torch.backends.cuda.enable_math_sdp(True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # moving to CUDA if possible"
      ],
      "metadata": {
        "id": "yX_54CKp96E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d-h2hEV70Wb"
      },
      "outputs": [],
      "source": [
        "encoder = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True) # loading MobileNetV2.\n",
        "\n",
        "encoder.classifier = nn.Identity() # removing the final classification layer to retrieve the feature map. Feature map: [1, 1280]\n",
        "encoder.to(device) # moving to CUDA if possible\n",
        "\n",
        "for param in encoder.parameters(): # freezing the encoder since we're not training it\n",
        "  param.requires_grad = False # avoid computing gradients for brevity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing transforms"
      ],
      "metadata": {
        "id": "VuZF5XjvZQrZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyf7gs9i6rIV"
      },
      "outputs": [],
      "source": [
        "transform_encoder = v2.Compose(\n",
        "    [\n",
        "        v2.Resize((224, 224)),\n",
        "        v2.SanitizeBoundingBoxes(), # removing invalid bounding boxes\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU-based decoder"
      ],
      "metadata": {
        "id": "IHbPya3xJiWu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0h9CFD09l8o"
      },
      "outputs": [],
      "source": [
        "class GRU_Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, feature_map_size=1280, embed_size=256, hidden_size=512, num_layers=2, vocab_size=10000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feature_map_size = feature_map_size\n",
        "    self.embed_size = embed_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size) # word --> embedding (vector representation)\n",
        "    self.proj = nn.Linear(in_features=self.feature_map_size, out_features=self.hidden_size) # dim(feature space) --> dim(hidden state) to draw caption-related information from the raw images\n",
        "    self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True) # embed_size --> hidden_size\n",
        "    self.fc = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size) # hidden state vector -> vocabulary vector (in hidden state vector space, the vector is not interpretable hence it needs to go to vocabulary vector space)\n",
        "    self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "\n",
        "  \"\"\" Called during val/test loop. Generates captions when passed BOS token and EOS token using autoregression not teacher-forcing. \"\"\"\n",
        "  def generate(self, feature_map, bos_token, eos_token, max_len=10):\n",
        "    batch_size = feature_map.size(0)\n",
        "    bos_token = self.embed(bos_token).unsqueeze(1)\n",
        "    h = self.proj(feature_map).unsqueeze(0) # initial hidden state\n",
        "\n",
        "    last_word = bos_token\n",
        "    caption = []\n",
        "\n",
        "    for i in range(max_len): # don't have the entire caption yet, so need to loop until its generated\n",
        "      output, h = self.gru(last_word, h) # passing the last word generated through the GRU layer to get the next word\n",
        "      output = self.dropout(output) # applying dropout to boost generalization\n",
        "      logits = self.fc(output) # now in vocabulary vector space\n",
        "      word = torch.argmax(logits, dim=1) # argmaxxing to get the most probable predicted word\n",
        "      caption.append(word) # adding this word to the caption generated so far\n",
        "\n",
        "      if torch.all(word == eos_token): # comparing word and eos token across the various dimensions\n",
        "        break # exit if reached end of caption\n",
        "\n",
        "      last_word = self.embed(word).unsqueeze(1) # shifting the last_word pointer to the right\n",
        "\n",
        "    return torch.stack(caption, dim=1) # formatting the caption correctly before returning it\n",
        "\n",
        "  \"\"\" Called during train loop. Generates captions when passed ground truth (words) using teacher-forcing not autoregression  \"\"\"\n",
        "  def forward(self, feature_map, words):\n",
        "    batch_size = feature_map.size(0)\n",
        "    embedding = self.embed(words) # returns a vector representation of a word\n",
        "    h0 = self.proj(feature_map).unsqueeze(0) # initializes the hidden state by projecting the feature map onto the hidden state dimensional space\n",
        "    h0 = h0.reshape(self.num_layers, batch_size, self.hidden_size) # gru expects hidden state in a certain format\n",
        "    output, _ = self.gru(embedding, h0) # teacher-forcing with the correct captions\n",
        "    logits = self.fc(output) # going from hidden state vector space --> vocabulary vector space\n",
        "\n",
        "    return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = GRU_Decoder(feature_map_size=FEATURE_MAP_SIZE, embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "decoder.to(device) # shifting model to CUDA if possible"
      ],
      "metadata": {
        "id": "KvHDkdK1-ifr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MiniCOCO: Karpathy Split of COCO\n",
        "\n",
        "This project uses the [Karpathy split](https://www.kaggle.com/datasets/shtvkumar/karpathy-splits/data) of the COCO dataset with the following customizations:\n",
        "\n",
        "- ✅ **Single caption per image** during training for faster convergence.\n",
        "- ✅ **Multiple captions per image** for validation and testing to enable robust evaluation.\n",
        "- ✅ **Special tokens** (`<bos>`, `<eos>`, `<pad>`) added to sequences to guide generation and training.\n",
        "- ✅ **Padding** applied to caption sequences to ensure uniform batch dimensions."
      ],
      "metadata": {
        "id": "F2mbIZNBJpEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE3zVz0Vwp5-"
      },
      "outputs": [],
      "source": [
        "class MiniCoco(Dataset):\n",
        "  def pad(data): # class function to pad captions for uniformity\n",
        "    images, captions = zip(*data) # unzipping data\n",
        "    images = torch.stack(images, dim=0) # combining along dim=0\n",
        "\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=train_data.vocab[\"<pad>\"])\n",
        "\n",
        "    return images, captions\n",
        "\n",
        "  def __init__(self, json_file, root_dir, split, transform=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.full_data = pd.read_json(json_file)\n",
        "    self.data = self.full_data[\"images\"]\n",
        "    self.split = split\n",
        "    self.counter = Counter() # counting the # of occurances of a particular word in a sentence\n",
        "    self.captions = [] # nested list with all the captions for each sample\n",
        "\n",
        "    if self.split == \"train\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"restval\"]\n",
        "    elif self.split == \"val\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"val\"]\n",
        "    elif self.split == \"test\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"test\"]\n",
        "    else:\n",
        "      raise Exception(\"Invalid split\")\n",
        "\n",
        "    self.length = len(self.data)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    if self.split == \"train\": # only want to pre-load captions for train -- during test/val mode they will be generated only for comparison with predicted\n",
        "      for sample_num in range(len(self.data)): # iterating over all samples in the train dataset\n",
        "        cap = [] # captions for particular sample\n",
        "\n",
        "        for j in range(len(self.data[sample_num][\"sentences\"])): # iterating over the various captions provided for each sample\n",
        "          caption = self.data[sample_num][\"sentences\"][j]\n",
        "          token = caption[\"tokens\"]\n",
        "          self.counter.update(token) # keeping track of the frequency of each token\n",
        "          cap.append(token)\n",
        "\n",
        "        self.captions.append(cap)\n",
        "\n",
        "    special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>'] # <unk>: unknown; <pad>: padding; <bos>: beginning of sentence; <eos>: end of sentence\n",
        "    self.vocab = vocab(self.counter, specials=special_tokens, special_first=True, min_freq=MIN_FREQUENCY) # defining vocab object for stoi and itos\n",
        "    self.vocab.set_default_index(self.vocab[\"<unk>\"]) # map to unk by default\n",
        "\n",
        "    for i in range(len(self.captions)):\n",
        "      for j in range(len(self.captions[i])):\n",
        "        self.captions[i][j] = self.encode(self.captions[i][j]) # mapping each caption in the nested list to an integer via encode()\n",
        "\n",
        "  def encode(self, text): # caption --> numerical representation\n",
        "    return [self.vocab[\"<bos>\"]] + [self.vocab.get_stoi()[s] for s in text] + [self.vocab[\"<eos>\"]]\n",
        "\n",
        "  def itos(self, tens):\n",
        "    return ' '.join(self.vocab.get_itos()[i] for i in tens[1:-1]) # applying itos for a 1D list of integers\n",
        "\n",
        "  def decode(self, ints):  # sequence of numbers --> space-seperated caption\n",
        "    if ints.dim() == 1:\n",
        "      return self.itos(ints)\n",
        "\n",
        "    return [self.itos(seq) for seq in ints] # applying itos for n-dim list\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # train images should only have 1 caption (leads to faster convergence when teacher-forcing during training)\n",
        "    if self.split == \"train\":\n",
        "      captions = self.captions[index][randint(0, len(self.captions[index]) - 1)]\n",
        "    # during test/val we will compare all 5 captions to the generated caption using bertscore for more accurate metrics\n",
        "    else:\n",
        "      captions = [self.encode(sent[\"tokens\"]) for sent in self.data[index][\"sentences\"]]\n",
        "\n",
        "    # storing the image into memory as a torch tensor\n",
        "    image_name = path.join(self.root_dir, self.data[index][\"filename\"])\n",
        "    image = decode_image(image_name, mode=\"RGB\") # returns tensor\n",
        "\n",
        "    return image, captions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data from Google Cloud Platform"
      ],
      "metadata": {
        "id": "VavHydxtWUB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "8-6iUJPPq2Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9stlTrjRqH6i"
      },
      "outputs": [],
      "source": [
        "def download_blob(bucket_name, source_blob_name, destination_file_name): # downloading the Karpathy split of the COCO dataset from Google Cloud Storage to Colab's VM\n",
        "\n",
        "  client = storage.Client(project=\"Image Captioning\")\n",
        "  bucket = client.bucket(bucket_name)\n",
        "  blob = bucket.blob(source_blob_name)\n",
        "  blob.download_to_filename(destination_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxmAMNnTrIEn"
      },
      "outputs": [],
      "source": [
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/test2014.zip\", \"/content/test2014.zip\")\n",
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/train2014.zip\", \"/content/train2014.zip\")\n",
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/val2014.zip\", \"/content/val2014.zip\")\n",
        "download_blob(\"img-captioning\", \"archive.zip\", \"/content/archive.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rVWta3qpwIH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!unzip /content/test2014.zip -d /content/test2014/ && unzip /content/train2014.zip -d /content/train2014/ && unzip /content/archive.zip -d /content/archive/ && !unzip /content/val2014.zip -d /content/val2014/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/test2014.zip /content/train2014.zip /content/val2014.zip /content/archive.zip"
      ],
      "metadata": {
        "id": "kWfX3ZuZv15w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/archive/dataset_flickr30k.json && rm /content/archive/dataset_flickr8k.json"
      ],
      "metadata": {
        "id": "a3aEx1bLYJ_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/archive && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3OgMABv9-9",
        "outputId": "d1e94749-ca3d-4680-a155-a4b10e8c931d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_coco.json  dataset_flickr30k.json  dataset_flickr8k.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/val/test splits"
      ],
      "metadata": {
        "id": "W81TWzSAXDzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = \"/content/archive/dataset_coco.json\"\n",
        "root_train_dir = \"/content/train2014/train2014/\"\n",
        "root_test_dir = \"/content/test2014/test2014/\"\n",
        "root_val_dir = \"/content/val2014/val2014/\"\n",
        "\n",
        "train_data = MiniCoco(json_file, root_train_dir, \"train\")\n",
        "test_data = MiniCoco(json_file, root_test_dir, \"test\")\n",
        "val_data = MiniCoco(json_file, root_val_dir, \"val\")"
      ],
      "metadata": {
        "id": "TdU9QiP6116j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=BATCHSIZE, shuffle=True, collate_fn=MiniCoco.pad) # padding all captions for uniformity because decoder performs better with captions of uniform length\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCHSIZE, shuffle=False, collate_fn=MiniCoco.pad)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCHSIZE, shuffle=False, collate_fn=MiniCoco.pad)"
      ],
      "metadata": {
        "id": "NBC3Nbm913Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of samples \\t Train: {len(train_dataloader) * BATCHSIZE}, Val: {len(val_dataloader) * BATCHSIZE}, Test: {len(test_dataloader) * BATCHSIZE}\")"
      ],
      "metadata": {
        "id": "IOaGIQNrsYqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train + val loop"
      ],
      "metadata": {
        "id": "Rc5986R1WzQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bertscore = load(\"bertscore\") # Using BERTScore as a metric because it prioritizes semeantic meaning between the predicted and true caption.\n",
        "loss_fn = nn.CrossEntropyLoss() # standard loss function for classification\n",
        "optimizer = optim.Adagrad(decoder.parameters()) # good for sparse data. Words are one-hot encoded so they're sparse. With embeddings, some words are infrequent, so they can be sparse too."
      ],
      "metadata": {
        "id": "F306irHEB9AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "  running_loss = 0\n",
        "  f1 = []\n",
        "\n",
        "  for batch_num, data in enumerate(train_dataloader):\n",
        "    images, captions = data\n",
        "\n",
        "    images = images.to(device)  # images: [B, C, H, W] batch, channels, height, width\n",
        "    captions = captions.to(device) # captions [B, L] batch, sequence length\n",
        "\n",
        "    sliced_captions = captions[:, :-1] # removing eos token\n",
        "    optimizer.zero_grad() # zeroing gradients because they accumulate\n",
        "\n",
        "    input_tensor = transform_encoder(images) # applying transformation and adding batch dimension\n",
        "    feature_map = encoder(input_tensor) # getting a feature map\n",
        "    outputs = decoder(feature_map, sliced_captions)\n",
        "\n",
        "    predicted_decoded = train_data.decode(outputs)\n",
        "    correct_decoded = train_data.decode(captions)\n",
        "    f1.append(score(predicted_decoded, correct_decoded, lang='en', verbose=True)) # getting the f1 score (precision + recall) using bertscore\n",
        "\n",
        "    loss = loss_fn(outputs, sliced_captions) # computing loss\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1) # gradient clipping to prevent exploding gradients\n",
        "    loss.backward() # backpropogating\n",
        "    running_loss += loss.item() # summing loss\n",
        "    optimizer.step()\n",
        "\n",
        "  avg_loss = running_loss / len(train_dataloader)\n",
        "  f1_avg = mean(f1)\n",
        "  return avg_loss, f1_avg"
      ],
      "metadata": {
        "id": "lJeRo02qHz0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate():\n",
        "  f1 = []\n",
        "\n",
        "  for vdata in val_dataloader:\n",
        "    vimages, vcaptions = vdata\n",
        "\n",
        "    # moving to CUDA\n",
        "    vimages = vimages.to(device)\n",
        "    vcaptions = vcaptions.to(device)\n",
        "\n",
        "    input_tensor = transform_encoder(vimages) # applying data cleaning transforms\n",
        "    feature_map = encoder(input_tensor) # getting feature map from encoder\n",
        "\n",
        "    # calling generate() which is essentially just __forward__ but is fed BOS instead of ground truth captions and the function has a slightly different implementation\n",
        "    voutputs = decoder.generate(feature_map, train_data.vocab[\"<bos>\"], train_data.vocab[\"<eos>\"], max_len=MAX_LEN)\n",
        "    vpredicted_decoded = train_data.decode(voutputs)\n",
        "    vcorrect_decoded = train_data.decode(vcaptions)\n",
        "    f1.append(score(vpredicted_decoded, vcorrect_decoded, lang='en', verbose=True))\n",
        "\n",
        "  f1_avg = mean(f1)\n",
        "  return f1"
      ],
      "metadata": {
        "id": "C7tAiLUEIj49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = -1\n",
        "loop = tqdm(range(EPOCHS)) # adding progress bar\n",
        "\n",
        "for epoch in loop:\n",
        "  decoder.train() # setting decoder to train mode\n",
        "  avg_loss, f1_train = train_one_epoch()\n",
        "\n",
        "  decoder.eval() # moving onto validation dataset\n",
        "\n",
        "  with torch.no_grad(): # no need to find gradients during validation\n",
        "    f1_val = validate() # validation accuracy\n",
        "\n",
        "  loop.set_description(f\"Avg Loss: {avg_loss} \\t Train F1 Score: {f1_train} \\t Val F1 Score: {f1_val}\")\n",
        "\n",
        "  if f1_val > best_acc: # if validation accuracy improves, save its progress\n",
        "    # NOTE: I used Accuracy over loss because its a better indicator of test accuracy (reflects generalization) and is less noisy.\n",
        "\n",
        "     torch.save({ # saving everything\n",
        "    'decoder_state_dict': decoder.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'vocab': train_data.vocab,\n",
        "    }, \"full_model_checkpoint.pt\")\n",
        "\n",
        "     best_acc = f1_val # update best accuracy\n"
      ],
      "metadata": {
        "id": "-XRud_Xv5AVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test loop"
      ],
      "metadata": {
        "id": "XliZ_3fYW8Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  f1 = []\n",
        "\n",
        "  for tdata in test_dataloader:\n",
        "    timages, tcaptions = tdata\n",
        "\n",
        "    # Moving to CUDA\n",
        "    timages = timages.to(device)\n",
        "    tcaptions = tcaptions.to(device)\n",
        "\n",
        "    input_tensor = transform_encoder(timages)\n",
        "    feature_map = encoder(input_tensor)\n",
        "\n",
        "    # calling generate() which is essentially just __forward__ but is fed BOS instead of ground truth captions and the function has a slightly different implementation\n",
        "    toutputs = decoder.generate(feature_map, train_data.vocab[\"<bos>\"], train_data.vocab[\"<eos>\"], max_len=MAX_LEN)\n",
        "    tpredicted_decoded = train_data.decode(toutputs)\n",
        "    tcorrect_decoded = train_data.decode(tcaptions)\n",
        "    f1.append(score(tpredicted_decoded, tcorrect_decoded, lang='en', verbose=True)) # aggregating the predictions and captions to evaluate with BERTScore each epoch\n",
        "\n",
        "  f1_avg = mean(f1)\n",
        "  return f1_avg"
      ],
      "metadata": {
        "id": "rUSY7L1NyLRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # no need to compute gradients during testing\n",
        "  decoder.eval() # switching to eval mode\n",
        "  print(test())"
      ],
      "metadata": {
        "id": "N15x6idyyFMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOdXlhDOcSeY1QTvVPa5MSl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}