{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/Image-Captioning/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkVD7rvu9p0-"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.17.0 && pip install torch==2.2.0 && pip install torchvision==0.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in3N6rxg7A8A"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from os import path\n",
        "from random import randint\n",
        "from collections import Counter\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d-h2hEV70Wb"
      },
      "outputs": [],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True) # feature map: [1, 1280]\n",
        "\n",
        "model.classifier = nn.Identity() # removing the final classification layer to retrieve the feature map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0h9CFD09l8o"
      },
      "outputs": [],
      "source": [
        "class GRU_Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, feature_map_size=1280, embed_size=256, hidden_size=512, num_layers=2, vocab_size=10000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feature_map_size = feature_map_size\n",
        "    self.embed_size = embed_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size) # to map word to a vector\n",
        "    self.proj = nn.Linear(in_features=self.feature_map_size, out_features=self.hidden_size) # to project the feature map onto the dimension space of the hidden state\n",
        "    self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size, num_layers=self.num_layers) # the gru layer\n",
        "    self.fc = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size) # to go from hidden state -> word\n",
        "\n",
        "  def forward(self, x, words, feature_map):\n",
        "    batch_size = feature_map.size(0)\n",
        "    words = self.embed(words) # returns a vector representation of a word\n",
        "    h0 = self.proj(feature_map).unsqueeze(0) # initializes the hidden state by projecting the feature map onto the hidden state dimensional space\n",
        "    h0 = h0.reshape(self.num_layers, batch_size, self.hidden_size) # gru expects hidden state in a certain format\n",
        "    output, _ = self.gru(words, h0) # teacher-forcing the correct captions and supplying the hidden state\n",
        "    logits = self.fc(output) # going from hidden state vector space --> word vector space\n",
        "\n",
        "    return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE3zVz0Vwp5-"
      },
      "outputs": [],
      "source": [
        "class MiniCoco(Dataset):\n",
        "\n",
        "  def __init__(self, json_file, root_dir, split=\"train\", transform=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.full_data = pd.read_json(json_file)\n",
        "    self.data = self.full_data[\"root\"][\"images\"]\n",
        "    self.split = split\n",
        "    self.counter = Counter()\n",
        "    self.captions = []\n",
        "\n",
        "    if self.split == \"train\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"restval\"]\n",
        "    elif self.split == \"val\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"val\"]\n",
        "    elif self.split == \"test\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"test\"]\n",
        "    else:\n",
        "      raise Exception(\"Invalid split\")\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    for i in range(len(self.data)):\n",
        "      cap = []\n",
        "\n",
        "      for j in range(len(self.data[i][\"sentences\"])):\n",
        "        caption = self.data[i][\"sentences\"][j][\"tokens\"]\n",
        "        self.counter.update(caption)\n",
        "        cap.append(caption)\n",
        "\n",
        "      self.captions.append(cap)\n",
        "\n",
        "    self.vocab = vocab.vocab(self.counter, min_freq=1)\n",
        "    self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "\n",
        "    for i in range(len(self.captions)):\n",
        "      for j in range(len(self.captions[i])):\n",
        "        self.captions[i][j] = self.encode(self.captions[i][j])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def encode(self, text):\n",
        "    return [self.vocab.get_stoi()[s] for s in text]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    captions = self.captions[index][randint(0, len(self.captions[index]) - 1)] if self.split == \"train\" else self.captions[index]\n",
        "\n",
        "    image_name = path.join(self.root_dir, self.data[index][\"filename\"])\n",
        "    image = io.imread(image_name)\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return image, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyf7gs9i6rIV"
      },
      "outputs": [],
      "source": [
        "transform = v2.Compose(\n",
        "    [\n",
        "        v2.Resize((224, 224)),\n",
        "        v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "8-6iUJPPq2Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9stlTrjRqH6i"
      },
      "outputs": [],
      "source": [
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "\n",
        "  client = storage.Client(project=\"Image Captioning\")\n",
        "  bucket = client.bucket(bucket_name)\n",
        "  blob = bucket.blob(source_blob_name)\n",
        "  blob.download_to_filename(destination_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KxmAMNnTrIEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2426084-5ca7-46d5-b52b-560e9720e0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/test2014.zip\", \"/content/test2014.zip\")\n",
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/train2014.zip\", \"/content/train2014.zip\")\n",
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/val2014.zip\", \"/content/val2014.zip\")\n",
        "download_blob(\"img-captioning\", \"archive.zip\", \"/content/archive.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rVWta3qpwIH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!unzip /content/test2014.zip -d /content/test2014/ && unzip /content/train2014.zip -d /content/train2014/ && unzip /content/archive.zip -d /content/archive/ && !unzip /content/val2014.zip -d /content/val2014/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/test2014.zip /content/train2014.zip /content/val2014.zip /content/archive.zip"
      ],
      "metadata": {
        "id": "kWfX3ZuZv15w"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3OgMABv9-9",
        "outputId": "ecab5c37-7bb7-4f0e-b750-e211ecc361c7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "archive  sample_data  test2014\ttrain2014  val2014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9um05Dw2o6p9"
      },
      "outputs": [],
      "source": [
        "json_file = \"/content/karpathy_split/dataset_coco.json\"\n",
        "root_train_dir = \"/content/train2014/train2014\"\n",
        "root_test_dir = \"/content/test2014/test2014\"\n",
        "root_val_dir = \"/content/val2014/val2014\"\n",
        "\n",
        "train_data = MiniCoco(json_file, root_train_dir, \"train\")\n",
        "test_data = MiniCoco(json_file, root_val_dir, \"val\")\n",
        "val_data = MiniCoco(json_file, root_test_dir, \"test\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnlZxq5IzejIK4rZBwvh0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}