{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/Image-Captioning/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkVD7rvu9p0-"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.17.0 && pip install torch==2.2.0 && pip install torchvision==0.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in3N6rxg7A8A"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.io import decode_image\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from os import path\n",
        "from random import randint\n",
        "from collections import Counter\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d-h2hEV70Wb"
      },
      "outputs": [],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True) # feature map: [1, 1280]\n",
        "\n",
        "model.classifier = nn.Identity() # removing the final classification layer to retrieve the feature map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "r0h9CFD09l8o"
      },
      "outputs": [],
      "source": [
        "class GRU_Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, feature_map_size=1280, embed_size=256, hidden_size=512, num_layers=2, vocab_size=10000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feature_map_size = feature_map_size\n",
        "    self.embed_size = embed_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size) # to map word to a vector\n",
        "    self.proj = nn.Linear(in_features=self.feature_map_size, out_features=self.hidden_size) # to project the feature map onto the dimension space of the hidden state\n",
        "    self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size, num_layers=self.num_layers) # the gru layer\n",
        "    self.fc = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size) # to go from hidden state -> word\n",
        "\n",
        "  def forward(self, x, words, feature_map):\n",
        "    batch_size = feature_map.size(0)\n",
        "    words = self.embed(words) # returns a vector representation of a word\n",
        "    h0 = self.proj(feature_map).unsqueeze(0) # initializes the hidden state by projecting the feature map onto the hidden state dimensional space\n",
        "    h0 = h0.reshape(self.num_layers, batch_size, self.hidden_size) # gru expects hidden state in a certain format\n",
        "    output, _ = self.gru(words, h0) # teacher-forcing the correct captions and supplying the hidden state\n",
        "    logits = self.fc(output) # going from hidden state vector space --> word vector space\n",
        "\n",
        "    return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lE3zVz0Vwp5-"
      },
      "outputs": [],
      "source": [
        "class MiniCoco(Dataset):\n",
        "\n",
        "  def __init__(self, json_file, root_dir, split, transform=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.full_data = pd.read_json(json_file)\n",
        "    self.data = self.full_data[\"images\"]\n",
        "    self.split = split\n",
        "    self.counter = Counter() # counting the # of occurances of a particular word in a sentence\n",
        "    self.captions = [] # nested list with all the captions for each sample\n",
        "\n",
        "    if self.split == \"train\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"restval\"]\n",
        "    elif self.split == \"val\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"val\"]\n",
        "    elif self.split == \"test\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"test\"]\n",
        "    else:\n",
        "      raise Exception(\"Invalid split\")\n",
        "\n",
        "    self.length = len(self.data)\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    if self.split == \"train\": # only want to store captions for train -- during test/val model should be generating without knowing any ground truth\n",
        "      for sample in range(len(self.data)): # iterating over all samples in the train dataset\n",
        "        cap = [] # captions for particular sample\n",
        "\n",
        "        for j in range(len(self.data[sample][\"sentences\"])): # iterating over the various captions provided for each sample\n",
        "          caption = self.data[sample][\"sentences\"][j]\n",
        "          token = caption[\"tokens\"]\n",
        "          self.counter.update(token) # keeping track of the frequency of each token\n",
        "          cap.append(token)\n",
        "\n",
        "        self.captions.append(cap)\n",
        "    else:\n",
        "      self.captions = None\n",
        "\n",
        "    special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "    self.vocab = vocab(self.counter, specials=special_tokens, special_first=True, min_freq=2) # mapping words to integers\n",
        "    self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "\n",
        "    for i in range(len(self.captions)):\n",
        "      for j in range(len(self.captions[i])):\n",
        "        self.captions[i][j] = self.encode(self.captions[i][j]) # mapping each caption in the nested list to an integer via encode()\n",
        "\n",
        "  def encode(self, text):\n",
        "    return [self.vocab[\"<bos>\"]] + [self.vocab.get_stoi()[s] for s in text] + [self.vocab[\"<eos>\"]]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # train images should only have 1 caption (more efficient when teacher-forcing)\n",
        "    captions = self.captions[index][randint(0, len(self.captions[index]) - 1)] if self.split == \"train\" else self.captions[index]\n",
        "\n",
        "    # storing the image into memory as a torch tensor\n",
        "    image_name = path.join(self.root_dir, self.data[index][\"filename\"])\n",
        "    image = decode_image(image_name, mode=\"RGB\")\n",
        "\n",
        "    return image, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyf7gs9i6rIV",
        "outputId": "c3c6091e-9d30-4966-8a91-bec8e9318fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "transform_encoder = v2.Compose(\n",
        "    [\n",
        "        v2.Resize((224, 224)),\n",
        "        v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_decoder = v2.Compose(\n",
        "    [\n",
        "        v2.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "OsyszOP30rIh"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "8-6iUJPPq2Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9stlTrjRqH6i"
      },
      "outputs": [],
      "source": [
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "\n",
        "  client = storage.Client(project=\"Image Captioning\")\n",
        "  bucket = client.bucket(bucket_name)\n",
        "  blob = bucket.blob(source_blob_name)\n",
        "  blob.download_to_filename(destination_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KxmAMNnTrIEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2426084-5ca7-46d5-b52b-560e9720e0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/test2014.zip\", \"/content/test2014.zip\")\n",
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/train2014.zip\", \"/content/train2014.zip\")\n",
        "download_blob(\"img-captioning\", \"images.cocodataset.org/zips/val2014.zip\", \"/content/val2014.zip\")\n",
        "download_blob(\"img-captioning\", \"archive.zip\", \"/content/archive.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rVWta3qpwIH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!unzip /content/test2014.zip -d /content/test2014/ && unzip /content/train2014.zip -d /content/train2014/ && unzip /content/archive.zip -d /content/archive/ && !unzip /content/val2014.zip -d /content/val2014/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/test2014.zip /content/train2014.zip /content/val2014.zip /content/archive.zip"
      ],
      "metadata": {
        "id": "kWfX3ZuZv15w"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/archive && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3OgMABv9-9",
        "outputId": "d1e94749-ca3d-4680-a155-a4b10e8c931d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_coco.json  dataset_flickr30k.json  dataset_flickr8k.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = \"/content/archive/dataset_coco.json\"\n",
        "root_train_dir = \"/content/train2014/train2014/\"\n",
        "root_test_dir = \"/content/test2014/test2014/\"\n",
        "root_val_dir = \"/content/val2014/val2014/\"\n",
        "\n",
        "train_data = MiniCoco(json_file, root_train_dir, \"train\", transform=transform_decoder)\n",
        "test_data = MiniCoco(json_file, root_test_dir, \"test\", transform=transform_decoder)\n",
        "val_data = MiniCoco(json_file, root_val_dir, \"val\", transform=transform_decoder)"
      ],
      "metadata": {
        "id": "TdU9QiP6116j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "NBC3Nbm913Eg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7/K28cUQxdXqSDYF7WQxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}