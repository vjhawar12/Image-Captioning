{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/Image-Captioning/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in3N6rxg7A8A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from os import path\n",
        "from random import randint\n",
        "from torchtext import vocab\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torchtext"
      ],
      "metadata": {
        "id": "TkVD7rvu9p0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d-h2hEV70Wb"
      },
      "outputs": [],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True) # feature map: [1, 1280]\n",
        "\n",
        "model.classifier = nn.Identity() # removing the final classification layer to retrieve the feature map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0h9CFD09l8o"
      },
      "outputs": [],
      "source": [
        "class GRU_Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, feature_map_size=1280, embed_size=256, hidden_size=512, num_layers=2, vocab_size=10000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feature_map_size = feature_map_size\n",
        "    self.embed_size = embed_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
        "    self.proj = nn.Linear(in_features=self.feature_map_size, out_features=self.hidden_size)\n",
        "    self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
        "    self.fc = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
        "\n",
        "  def forward(self, x, words, feature_map):\n",
        "    batch_size = feature_map.size(0)\n",
        "    words = self.embed(words)\n",
        "    h0 = self.proj(feature_map).unsqueeze(0)\n",
        "    h0 = h0.reshape(self.num_layers, batch_size, self.hidden_size)\n",
        "    output, _ = self.gru(words, h0)\n",
        "    logits = self.fc(output)\n",
        "\n",
        "    return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniCoco(Dataset):\n",
        "\n",
        "  def __init__(self, json_file, root_dir, split=\"train\", transform=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.full_data = pd.read_json(json_file)\n",
        "    self.data = self.full_data[\"root\"][\"images\"]\n",
        "    self.split = split\n",
        "    self.counter = Counter()\n",
        "    self.captions = []\n",
        "\n",
        "    if self.split == \"train\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"restval\"]\n",
        "    elif self.split == \"val\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"val\"]\n",
        "    elif self.split == \"test\":\n",
        "      self.data = [obj for obj in self.data if obj[\"split\"] == \"test\"]\n",
        "    else:\n",
        "      raise Exception(\"Invalid split\")\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    for i in range(len(self.data)):\n",
        "      cap = []\n",
        "\n",
        "      for j in range(len(self.data[i][\"sentences\"])):\n",
        "        caption = self.data[i][\"sentences\"][j][\"tokens\"]\n",
        "        self.counter.update(caption)\n",
        "        cap.append(caption)\n",
        "\n",
        "      self.captions.append(cap)\n",
        "\n",
        "    self.vocab = vocab.vocab(self.counter, min_freq=1)\n",
        "    self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "\n",
        "    for i in range(len(self.captions)):\n",
        "      for j in range(len(self.captions[i])):\n",
        "        self.captions[i][j] = self.encode(self.captions[i][j])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def encode(self, text):\n",
        "    return [self.vocab.get_stoi()[s] for s in text]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    captions = self.captions[index][randint(0, len(self.captions[index]) - 1)] if self.split == \"train\" else self.captions[index]\n",
        "\n",
        "    image_name = path.join(self.root_dir, self.data[index][\"filename\"])\n",
        "    image = io.imread(image_name)\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return image, captions"
      ],
      "metadata": {
        "id": "lE3zVz0Vwp5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyf7gs9i6rIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c12486-bf29-400e-94e0-5e1dfbc808c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "transform = v2.Compose(\n",
        "    [\n",
        "        v2.Resize((224, 224)),\n",
        "        v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAj6+8epYafGmc8YPuRgrm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}